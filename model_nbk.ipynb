{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Main(), for V4529\n",
      "\n",
      "Examine: V4529\n",
      "count    2825.000000\n",
      "mean       35.463009\n",
      "std        19.589198\n",
      "min         1.000000\n",
      "25%        11.000000\n",
      "50%        40.000000\n",
      "75%        54.000000\n",
      "max        57.000000\n",
      "Name: V4529, dtype: float64\n",
      "\n",
      "crime_cat., frequency -  (1, 75) :  2.7 % \t-  Sexual assault or rape\n",
      "crime_cat., frequency -  (5, 72) :  2.5 % \t-  Robbery with assault\n",
      "crime_cat., frequency -  (7, 131) :  4.6 % \t-  Robbery without assault\n",
      "crime_cat., frequency -  (40, 249) :  8.8 % \t-  Car theft\n",
      "crime_cat., frequency -  (11, 533) :  18.9 % \t-  Assault\n",
      "crime_cat., frequency -  (21, 57) :  2.0 % \t-  Pickpocketing or purse snatching\n",
      "crime_cat., frequency -  (54, 597) :  21.1 % \t-  Theft under $250\n",
      "crime_cat., frequency -  (57, 582) :  20.6 % \t-  Theft over $250\n",
      "crime_cat., frequency -  (31, 529) :  18.7 % \t-  Home invasion theft\n",
      "Canvas is accessible via web browser at the URL: http://localhost:37088/index.html\n",
      "Opening Canvas in default web browser.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Analizer at 0x7ffaacf4dfd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import graphlab as gl\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "class Analizer(object):\n",
    "\n",
    "    def __init__(self, new_file='data/5-Data.tsv', new_predict = 'V4529', verbose=True):\n",
    "        '''\n",
    "        What: initialize various objects for making a graphlab boosted trees classifier. Calls _main()\n",
    "\n",
    "        Inputs: file location for data.tsv, which is dataset with 10,000 entries and 1100+ features\n",
    "        string name of feature I want to make a new model for. See self.code_dict for looking up feature codes\n",
    "        Outputs: a pandas dataframe based on input file, and a dictionary for looking up feature codes.\n",
    "        '''\n",
    "\n",
    "\n",
    "        self.df_data = pd.read_table(new_file)\n",
    "\n",
    "        self.code_dict = self.codebookIntoDict()\n",
    "        self.crime_dict = {1: \"Sexual assault or rape\", 5: \"Robbery with assault\", 7: \"Robbery without assault\",\n",
    "                           11: \"Assault\", # 12: \"Assault with a weapon\", 16: \"Unwanted sexual contact without force\",\n",
    "                           21: \"Pickpocketing or purse snatching\", 31: \"Home invasion theft\", 40: \"Car theft\",\n",
    "                           54: \"Theft under $250\", 57: \"Theft over $250\"} # , -1: \"Not defined / error\"}\n",
    "\n",
    "        path = 'feature_importanceSK_V4529.p'\n",
    "\n",
    "        if os.path.isfile(path) == True:\n",
    "\n",
    "            self.old_feature_importance = pk.load(open(path, 'r'))\n",
    "\n",
    "            if verbose == False:\n",
    "                print \"\\n old_feature_importance: \\n\"\n",
    "\n",
    "                for classification in self.old_feature_importance:\n",
    "\n",
    "                    class_dict = dict(classification[1]).keys()\n",
    "                    print classification[0], ':\\t',  class_dict\n",
    "                    self.printCodes(class_dict)\n",
    "\n",
    "        else:\n",
    "            self.old_feature_importance = None\n",
    "            print 'Warning: missing file  ' + path + ' , code may not work.'\n",
    "\n",
    "        self.df_small = None\n",
    "        self.model = None\n",
    "        self.model_small = None\n",
    "        self.predictions = None\n",
    "        self.last_results = None\n",
    "\n",
    "        self.predict = new_predict\n",
    "\n",
    "        self._main()\n",
    "\n",
    "\n",
    "    def _main(self):\n",
    "        '''\n",
    "        What: This function calls the data hygiene, simple feature engineering, and model constuction methods;\n",
    "        and prints the results\n",
    "\n",
    "        Inputs: receives initial data from __init__\n",
    "        Outputs: print statements\n",
    "        '''\n",
    "        # self.printSurveyAbstract()\n",
    "\n",
    "        print (\"\\n Main(), for \" + self.predict + '\\n')\n",
    "\n",
    "        self.trimFeatures(verbose=True)\n",
    "        self.compressV4529()\n",
    "        self.correctFeatureImbalance()\n",
    "        self.examineFeature(self.predict)\n",
    "\n",
    "        gl.canvas.set_target('browser')\n",
    "\n",
    "        gl.SFrame(self.df_data).show()\n",
    "\n",
    "\n",
    "        # this method takes ~35mins to execute\n",
    "        # self.computeFeatureImportance([1,5,7,11,21,31,40,54,57])\n",
    "\n",
    "        # self.predictFeatureGL(GLiters=6, feature_slice=20)\n",
    "\n",
    "\n",
    "    def compressV4529(self):\n",
    "        '''\n",
    "        What: squish down many similiar types of crime.\n",
    "                IE, (assault,completed + assault,attempted = assault)\n",
    "\n",
    "        In: self.df_data\n",
    "        Out: modifies self.df_data['V4529'] to have less variance\n",
    "        '''\n",
    "        # make an empty list\n",
    "        values = [None] * len(self.df_data['V4529'])\n",
    "\n",
    "        for i, oldCode in enumerate(self.df_data['V4529']):\n",
    "            crimeCode='unassigned'\n",
    "\n",
    "            '''\n",
    "            if int(oldCode) in [1, 2, 3, 4, 15, 18, 19]:\n",
    "                crimeCode = 'Sexual assault'  # turn rape and sexual assault into one cat.\n",
    "\n",
    "            elif int(oldCode) in [5, 6, 8, 9]:\n",
    "                crimeCode = 'Theft with assault'  # theft with assault'\n",
    "\n",
    "            elif int(oldCode) in [7, 10]:\n",
    "                crimeCode = 'Theft without assault'  # 'robbery withOUT assault'\n",
    "\n",
    "            elif int(oldCode) in [11, 14, 17, 20]:\n",
    "                crimeCode = 'Assault'  # assault\n",
    "\n",
    "            elif int(oldCode) in [12, 13]:\n",
    "                crimeCode = 'Assault with weapon'  # assault with a Weapon\n",
    "\n",
    "            elif int(oldCode) in [16]:\n",
    "                crimeCode = 'Unwanted sexual contact'  # unwanted sexual contact without force\n",
    "\n",
    "            elif int(oldCode) in [21, 22, 23]:\n",
    "                crimeCode = 'P.pocketing/purse-snatching'  # pickpocketing / purse snatching\n",
    "\n",
    "            elif int(oldCode) in [31, 32, 33]:\n",
    "                crimeCode = 'Home invasion/theft'  # home invasion burglary\n",
    "\n",
    "            elif int(oldCode) in [40, 41]:\n",
    "                crimeCode = 'Car theft'  # car theft\n",
    "\n",
    "            elif int(oldCode) in [54, 55, 56, 58, 59]:\n",
    "                crimeCode = 'Theft under $250'  # theft under $250\n",
    "\n",
    "            elif int(oldCode) in [57]:\n",
    "                crimeCode = 'Theft over $250'  # theft over $250\n",
    "            '''\n",
    "\n",
    "\n",
    "            if int(oldCode) in [1,2,3,4,15,16,18,19]:\n",
    "                crimeCode = 1 # turn rape and sexual assault into one catagory\n",
    "\n",
    "            elif int(oldCode) in [5,6,8,9]:\n",
    "                crimeCode = 5 # robbery with assault'\n",
    "\n",
    "            elif int(oldCode) in [7, 10]:\n",
    "                crimeCode = 7  # 'robbery withOUT assault'\n",
    "\n",
    "            elif int(oldCode) in [11,14,17,20, 12,13]:\n",
    "                crimeCode = 11  # assault\n",
    "\n",
    "            #elif int(oldCode) in [12,13]:\n",
    "                #crimeCode = 12  # assault with a Weapon\n",
    "\n",
    "            #elif int(oldCode) in [16]:\n",
    "                #crimeCode = 16  # unwanted sexual contact without force\n",
    "\n",
    "            elif int(oldCode) in [21,22,23]:\n",
    "                crimeCode = 21  # pickpocketing / purse snatching\n",
    "\n",
    "            elif int(oldCode) in [31,32,33]:\n",
    "                crimeCode = 31  # home invasion burglary\n",
    "\n",
    "            elif int(oldCode) in [40,41]:\n",
    "                crimeCode = 40  # car theft\n",
    "\n",
    "            elif int(oldCode) in [54,55,56,58,59]:\n",
    "                crimeCode = 54  # theft under $250\n",
    "\n",
    "            elif int(oldCode) in [57]:\n",
    "                crimeCode = 57  # theft over $250\n",
    "\n",
    "\n",
    "            values[i] = crimeCode\n",
    "\n",
    "        del self.df_data['V4529']\n",
    "        self.df_data['V4529'] = values\n",
    "\n",
    "\n",
    "    def correctFeatureImbalance(self, classes=[54, 57, 31, 11, 21, 1, 5, 7, 40], ratios=[.15, .4, .4, .4]):\n",
    "\n",
    "        sf_new = None\n",
    "\n",
    "        sf = gl.SFrame(self.df_data)\n",
    "\n",
    "        for i, crime_class in enumerate(classes):\n",
    "\n",
    "            if i <= 3:\n",
    "                slice = sf[sf[self.predict] == crime_class].sample(ratios[i])  # down sample unbalanced classes\n",
    "            else:\n",
    "                slice = sf[sf[self.predict] == crime_class]  # take all these (more rare) crimes\n",
    "\n",
    "            if sf_new is None:\n",
    "                sf_new = slice\n",
    "            else:\n",
    "                sf_new = sf_new.append(slice)\n",
    "\n",
    "            #print \"correctFeatureImbalance: \", i, self.crime_dict[crime_class]\n",
    "\n",
    "        self.df_data = sf_new.to_dataframe()\n",
    "\n",
    "\n",
    "    def trimFeatures(self, verbose=False):\n",
    "        low_var_features = ['V2001', 'V2009', 'V2027', 'V2028', 'V2029', 'V2030', 'V2031', 'V2109',\n",
    "                            'V2110', 'V2112', 'V2114', 'V2115', 'V2123', 'V2131', 'V2142', 'V3001',\n",
    "                            'V3027', 'V3051', 'V3057', 'V3060', 'V3069', 'V3082', 'V4001', 'V4319',\n",
    "                            'V4320',\n",
    "                            'V2060', 'V2061', 'V2062', 'V4313'\n",
    "                            ]\n",
    "\n",
    "        overpowered_features = ['V4528', 'V4526', 'V4350', # 4526/8/9, type of crime\n",
    "            'V4140B1', 'V4140B2', 'V4140B3', 'V4140B10', # 'did you feel x ?'\n",
    "            'V4024', 'V4074', 'V4075', 'V4359', 'V4311', 'V4375', 'V4352', 'V4076', 'V4376',\n",
    "            'V4002', 'V4048', 'V4049', 'V4073', 'V4092', 'V4097', 'V4098', 'V4099', 'V4100', 'V4101', # 4097, attacked: shot\n",
    "            'V4102', 'V4103', 'V4104', 'V4105', 'V4106', 'V4107', 'V4108', 'V4109', 'V4111', 'V4123',\n",
    "            'V4059', 'V4093', 'V4062', 'V4005', 'V4071', 'V4077', 'V4060', 'V4061', 'V4096', 'V4127', 'V4040',\n",
    "            'V4112', 'V4094', 'V4364', 'V4321', 'V4287', 'V4288', 'V4289', 'V4028', 'V4027', 'V4029',\n",
    "            'V4373', 'V4290', 'V4012', 'V4026', 'V4095', 'V4078', 'V4079', 'V4080', 'V4081', 'V4082', 'V4161',\n",
    "            'V4050', # 4050, what was weapon\n",
    "            'V4404', 'V4405', 'V4406', 'V4407', 'V4408', 'V4409', 'V4410', 'V4411', 'V4412', 'V4413',\n",
    "            'V4414', 'V4415', 'V4416', 'V4417', 'V4418', 'V4419', 'V4420', 'V4421', # 4404-4420, reason not reported\n",
    "            'V4522A', 'V4522B', 'V4522C', 'V4522D', 'V4522E', 'V4522F',# customer or client\n",
    "            'V4291', 'V4292', 'V4293', 'V4294', 'V4295', 'V4296', 'V4297', 'V4298', 'V4299', 'V4314',\n",
    "            'V4310', 'V4358', 'V4162', 'V4110', 'V4381', 'V4371',\n",
    "            'V4322', 'V4323', 'V4324', 'V4326', 'V4351', 'V4357', 'V4385', 'V4397', 'V4360', 'V4317',\n",
    "            'V4063', 'V4064', 'V4065', 'V4066', 'V4067', 'V4068', 'V4069', 'V4070', #these are all targets\n",
    "            'V4422', 'V4011', 'V4423', 'V4426', # reason (or not) reported\n",
    "            'V3014', 'V3080', 'V3026', 'V3002', 'V3008', 'V3013', 'V3005',\n",
    "            'V2008', 'V2002', 'V2116', 'V2117', 'V2118', 'V2023', 'V2033', 'V2005', 'V2006', 'V2012', 'V2016',\n",
    "            'V2011', 'V2014', 'V2073', 'V2022', 'V2024', # housing info\n",
    "            # 'INCREPWGT148', 'INCREPWGT40', 'INCREPWGT2', 'INCREPWGT3', 'INCREPWGT51',\n",
    "            # 'INCREPWGT14', 'INCREPWGT43', 'INCREPWGT57', 'INCREPWGT83',\n",
    "            'FRCODE', 'WGTPERCY', 'WGTHHCY', 'IDPER', 'IDHH', 'V4008', 'YEARQ', 'V2003','V2004']  #\n",
    "\n",
    "        strong_features_from_regression = [\n",
    "        'V4450', 'V4451', 'V4452', 'V4453', 'V4454', 'V4455', 'V4438',\n",
    "        'V4456', 'V4457', 'V4458', 'V4459', # police responses\n",
    "        'V4140B25', 'V4140B24', 'V4140B27', 'V4140B26', 'V4140B21', 'V4140B20',\n",
    "        'V4140B23', 'V4140B22', 'V4089', 'V4088', 'V4087', 'V4086', 'V4085', 'V4084', 'V4083', 'V4090', 'V4091',\n",
    "        'V4032', 'V4033', 'V4030', 'V4031', 'V4036', 'V4037', 'V4034', 'V4035',\n",
    "        'V4522G', 'V4522I', 'V4522H', #relation to attacker\n",
    "        'V4267', 'V4266', 'V4265', 'V4263', 'V4262', 'V4269', # relations\n",
    "        'V4519', 'V4518', 'V4264', 'V4261', 'V4512', 'V4513', 'V4515', 'V4516', #relations\n",
    "        'V4308', 'V4309', 'V4304', 'V4305', 'V4306', 'V4307', 'V4300', 'V4301', 'V4302', 'V4303',\n",
    "        'V4340', 'V4341', 'V4342', 'V4343', 'V4344', 'V4345', 'V4346', 'V4347', 'V4348', 'V4349', # items taken\n",
    "        'V4327', 'V4325', 'V4047', 'V4046', 'V4374', 'V4044', 'V4045', 'V4367', 'V4368', 'V4369',\n",
    "        'V4335', 'V4334', 'V4337', 'V4336', 'V4331', 'V4330', 'V4333', 'V4332', 'V4234', 'V4370', 'V4372', 'V4339', 'V4338', # items taken\n",
    "        'V4366', 'V4208', 'V4328', 'V4329', 'V4386A', 'V4365', # items taken\n",
    "        'V2128B','V2135', 'V2134', 'V2137', 'V2136', 'V2038', 'V2130', # panel meta data\n",
    "        'V4141', 'V4143', 'V4146', 'V4147', 'V4149', # victim self defense actions\n",
    "        'V4425', 'V4424', #reason reported\n",
    "        'V2133',\n",
    "        'V4157', 'V4159', # victim actions taken\n",
    "        'V4120', 'V4121', 'V4122',  # victim injuries\n",
    "        'V4140B6', 'V4140B7', 'V4140B4', 'V4140B5', 'V4140B8', 'V4140B9', 'V4140B12', 'V4140B13',\n",
    "        'V4140B11', # 'did you feel X?'\n",
    "        'V3055', # first incident\n",
    "        'V4355', 'V3056', 'V4072', # 'car' in description\n",
    "        'VICREPWGT71', 'VICREPWGT70', 'VICREPWGT73', 'VICREPWGT72', 'VICREPWGT75', 'VICREPWGT74',\n",
    "        'VICREPWGT77', 'VICREPWGT76', #replicate weight, method for accounting for survey data\n",
    "        'V3047'\n",
    "        ]\n",
    "\n",
    "        if self.predict in overpowered_features:\n",
    "            overpowered_features.remove(self.predict)\n",
    "            \n",
    "        if self.predict in strong_features_from_regression:\n",
    "            strong_features_from_regression.remove(self.predict)\n",
    "\n",
    "        if verbose is False:\n",
    "            print \"Over powered features: \\n\"\n",
    "            self.printCodes(overpowered_features)\n",
    "            self.printCodes(strong_features_from_regression)\n",
    "\n",
    "        self.delFeatures(low_var_features)\n",
    "        self.delFeatures(overpowered_features)\n",
    "        self.delFeatures(strong_features_from_regression)\n",
    "\n",
    "\n",
    "    def examineFeature(self, feature='V4529'):\n",
    "\n",
    "        print \"Examine: \" + feature\n",
    "\n",
    "        df_examine = self.df_data[feature]\n",
    "\n",
    "        print str(df_examine.describe()) + \"\\n\"\n",
    "\n",
    "        freq = Counter(list(df_examine.values))\n",
    "\n",
    "        for item in freq.items():\n",
    "            percent = np.round((float(item[1]) / self.df_data[self.predict].count()) * 100, 1)\n",
    "            print \"crime_cat., frequency - \", item,\": \", percent, \"% \\t- \", self.crime_dict[item[0]]\n",
    "\n",
    "\n",
    "    def testForDataLeakage(self, typeOfCrime=1, top_n=20):\n",
    "\n",
    "        feature_aucs = {}\n",
    "\n",
    "        for feature in list(self.df_data.columns.values):\n",
    "\n",
    "            if str(feature) == 'V4529':\n",
    "                pass\n",
    "            else:\n",
    "                model_cur = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "                cur_X = self.df_data[feature].values\n",
    "                cur_X = cur_X.reshape(cur_X.shape[0], 1)\n",
    "\n",
    "                cur_y = self.df_data[self.predict].apply(lambda x: 1 if x==typeOfCrime else 0).values\n",
    "\n",
    "                auc_cur = cross_val_score(model_cur, cur_X, cur_y, n_jobs=-1)\n",
    "\n",
    "                feature_aucs[feature] = np.round(np.mean(auc_cur), 4)\n",
    "\n",
    "        auc_counter = Counter(feature_aucs).most_common(top_n)\n",
    "\n",
    "        return list(auc_counter)\n",
    "\n",
    "\n",
    "    def computeFeatureImportance(self, crimes = [12, 31]):\n",
    "        \"\"\"\n",
    "        What: Test all features versus all classes (1 class at a time)\n",
    "            and record the results in a pickle\n",
    "\n",
    "        In: crime codes that you want to test, self.df_data\n",
    "        Out: pickle file, written to file\n",
    "        \"\"\"\n",
    "\n",
    "        crime_to_features_causation = []\n",
    "        print \"\\n--------------------------\"\n",
    "        print '\\ncomputeFeatureImportance():  ', str(len(crimes)), \" crime categories\"\n",
    "        print \"\\t Matrix shape: \", str(self.df_data.shape), '\\n'\n",
    "\n",
    "        for i, crime in enumerate(crimes):\n",
    "            print \"\\t\", str(int(i)+1), self.crime_dict[crime], \"\\n\"\n",
    "            top_features = self.testForDataLeakage(crime, 50)\n",
    "            crime_to_features_causation.append((self.crime_dict[crime], top_features))\n",
    "\n",
    "        for correlation in crime_to_features_causation:\n",
    "            print correlation, '\\n'\n",
    "\n",
    "\n",
    "        if os.path.isfile('feature_importance_sk.p') == True:\n",
    "            os.remove('feature_importance_sk.p')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        f = open('feature_importance_sk.p', 'w')\n",
    "        pk.dump(crime_to_features_causation, f)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    def printCrimeDict(self):\n",
    "        print \"\\n\"\n",
    "        for item in self.crime_dict.items():\n",
    "            print item\n",
    "\n",
    "\n",
    "    def regressOnFeatureSM(self):\n",
    "        '''\n",
    "        What: create a regression model with Statsmodels and try and find the Betas,\n",
    "        ie, try to find feature causation.\n",
    "\n",
    "        In: trimmed-down data\n",
    "        Out: print statements and a linear model regression\n",
    "        '''\n",
    "        df_dummies = pd.get_dummies(self.df_data)\n",
    "\n",
    "        df_dummies = pd.concat([df_dummies, self.df_data], axis=0)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df_dummies, self.df_data[self.predict].values,\n",
    "                                                            test_size=0.3, random_state=42)\n",
    "        X_train = tools.add_constant(X_train)\n",
    "        modelSM = Logit(y_train, X_train).fit()\n",
    "\n",
    "\n",
    "\n",
    "        print modelSM.summary()\n",
    "\n",
    "\n",
    "    def predictFeatureGL(self, GLiters=4, feature_slice=20):\n",
    "        '''\n",
    "        What: Does model constuction methods; and prints the results\n",
    "\n",
    "        Inputs: receives initial data from __init__\n",
    "        Outputs: print statements, model -> self.model\n",
    "        '''\n",
    "\n",
    "        self.model = self.doOneGLModel(self.predict, self.df_data, iters=GLiters)\n",
    "        self.printResults(\"big\")\n",
    "\n",
    "        if self.predict == 'V4529':\n",
    "            self.printCrimeDict()\n",
    "\n",
    "        # make a slice of df_data with given feature_importance and then make a model with just those features\n",
    "        for n_features in [5, 10, 15, feature_slice]:\n",
    "            print \"\\n\\n<><><><><><><><><><><> n_features: \", n_features, \" <><><><><><><><><><><><><><>\\n\"\n",
    "            self.df_small = self.getTopNfeatures(n_features)\n",
    "            self.model_small = self.doOneGLModel(self.predict, self.df_small, iters=GLiters*5)\n",
    "            self.printResults(\"small\")\n",
    "\n",
    "            # self.getTopNfeatures(n_features)\n",
    "\n",
    "\n",
    "    def codebookIntoDict(self, my_file='data/data_meta/BIG-Codebook.txt'):\n",
    "\n",
    "        '''\n",
    "        What: Turns codebook.txt into a dictionary, when can be accessed with self.code_dict\n",
    "\n",
    "        Inputs: file location of codebook.txt\n",
    "        Outputs: dictionary of all 1100+ features stored into self.code_dict\n",
    "        '''\n",
    "\n",
    "        with open(my_file) as f:\n",
    "            c_temp = f.readlines()\n",
    "\n",
    "        codebook = c_temp[10880:40820]\n",
    "        keys_values = []\n",
    "\n",
    "        for line in codebook:\n",
    "\n",
    "            matchObj = re.match('V[1-4]...', line, re.I)\n",
    "            matchObj2 = re.match('INC', line, re.I)\n",
    "\n",
    "            if matchObj and \"-\" in line:\n",
    "                keys_values.append(line)\n",
    "            if matchObj2:\n",
    "                keys_values.append(line)\n",
    "\n",
    "        # take off the last N values\n",
    "        offset = 0\n",
    "        keys_shorter = keys_values[0:len(keys_values)- offset]\n",
    "\n",
    "        key_value_pairs = []\n",
    "\n",
    "        for line in keys_shorter:\n",
    "            # V4004 - D Assault: attempted\n",
    "            if \"-\" in line:\n",
    "                ll = line.replace(\"\\n\", \"\")\n",
    "                sp = ll.split(' - ')\n",
    "                key_value_pairs.append(sp)\n",
    "\n",
    "        code_dict = {}\n",
    "        keys_temp = code_dict.keys()\n",
    "\n",
    "        for line in key_value_pairs:\n",
    "            if len(line) > 1 and line[0] not in keys_temp:\n",
    "                code_dict[line[0]] = line[1]\n",
    "\n",
    "        return code_dict\n",
    "\n",
    "\n",
    "    def delFeatures(self, features):\n",
    "        for cur_feat in features:\n",
    "            del self.df_data[cur_feat]\n",
    "\n",
    "\n",
    "    def doOneGLModel(self, predict, df_to_predict_on, iters=5):\n",
    "        '''\n",
    "        What: Make a predictive model using a graphlab boosted trees classifier.\n",
    "\n",
    "        Inputs: string predict, which is the feature we want to make a model of.\n",
    "        pandas dataframe df_to_predict_on\n",
    "        Outputs: model object\n",
    "        various print statements\n",
    "        '''\n",
    "        sf_data = gl.SFrame(df_to_predict_on)\n",
    "\n",
    "        if predict in self.code_dict.keys():\n",
    "            print (\"\\n\" + predict + \" is in the dictionary; \" + predict + \" = \" + self.code_dict[predict] + '\\n')\n",
    "\n",
    "        # graphlab.boosted_trees_classifier.create(dataset, target, features=None,\n",
    "        # max_iterations=10, validation_set='auto', class_weights=None, max_depth=6,\n",
    "        # step_size=0.3, min_loss_reduction=0.0, min_child_weight=0.1, row_subsample=1.0,\n",
    "        # column_subsample=1.0, verbose=True, random_seed=None, metric='auto', **kwargs)\n",
    "\n",
    "        model = gl.boosted_trees_classifier.create(sf_data, predict, features=None,\n",
    "                max_iterations=iters, validation_set='auto', class_weights=None, max_depth=15,\n",
    "                step_size=0.3, min_loss_reduction=0.0, min_child_weight=0.1, row_subsample=1.0,\n",
    "                column_subsample=1.0, verbose=True, random_seed=42, metric='auto')\n",
    "\n",
    "        self.predictions = model.predict(sf_data)\n",
    "        self.last_results = model.evaluate(sf_data)\n",
    "        print (\"\\n -> Finished computing model.\")\n",
    "        print (\"\\n<------------------------------------------------------------->\")\n",
    "        print (\"Results for \" + predict + \" \" + self.code_dict[predict] + \" :\\n\")\n",
    "        print (\"<------------------------------------------------------------->\")\n",
    "        return model\n",
    "\n",
    "\n",
    "    def printResults(self, size=\"big\"):\n",
    "        print \"\\nFeature importance: \\n\"\n",
    "\n",
    "        if size==\"big\":\n",
    "            print self.model.get_feature_importance()\n",
    "        else:\n",
    "            print self.model_small.get_feature_importance()\n",
    "\n",
    "        print(\"\\nModel results: \\n\")\n",
    "\n",
    "        print self.last_results\n",
    "\n",
    "\n",
    "    def printCodes(self, codes):\n",
    "        for i, code in enumerate(codes):\n",
    "            if code in self.code_dict.keys():\n",
    "                print code + \", \" + self.code_dict[code]\n",
    "            else:\n",
    "                print code\n",
    "        print \"\"\n",
    "\n",
    "\n",
    "    def getTopNfeatures(self, top_n_features):\n",
    "        '''\n",
    "        What: After I compute a model for all features, then go and create a new pandas dataframe, df_small,\n",
    "         to be used to re-make the model with only the top N most important features.\n",
    "\n",
    "        Inputs: self.model, N number of most important features to recompute with\n",
    "        Outputs: a pandas dataframe, df_small, which is a subset of self.df_data, including only the top N features\n",
    "        various print statements\n",
    "        '''\n",
    "\n",
    "        features_small = list(self.model.get_feature_importance()['name'][0:top_n_features])\n",
    "        print (\"\\t Top \" + str(top_n_features) + \" features of model: \\n\")\n",
    "\n",
    "        self.printCodes(features_small)\n",
    "\n",
    "        # 4528, type of crime\n",
    "        features_small.append(self.predict)\n",
    "\n",
    "        df_small = self.df_data[features_small]\n",
    "\n",
    "        return df_small\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Analizer(new_predict='V4529', verbose=True)\n",
    "# V4529 type of crime\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
